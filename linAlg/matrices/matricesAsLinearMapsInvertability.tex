\subsection{Invertiblity}

\subsubsection{Definition}
We can represent a system of linear equations using matrices.

\begin{example}
	\begin{equation*}
		\begin{cases}
			x  + 2y + 3z &= 8  \\
			-x      + 4z &= 12 \\
			2x - y  +  z &= 0 
		\end{cases} \rightarrow
		\begin{bmatrix}
			1  & 2  & 3 \\
			-1 & 0  & 4 \\
			2  & -1 & 1 
		\end{bmatrix}\begin{bmatrix}
			x \\ y \\ z
		\end{bmatrix} = \begin{bmatrix}
			8 \\ 12 \\ 0
		\end{bmatrix}.
	\end{equation*}
\end{example}

To solve such a system using matrices, we'd like a matrix that we could multiply both sides of the equation on the left by that multiplies with the existing matrix to give an identity matrix.
Then we'd be left with an equation like $\langle x, y, z \rangle = \langle \dots \rangle$, which would give us a solution.

\begin{definition}
	A matrix $A$ is invertible if there exists a matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.
	Such a matrix is called the inverse of $A$.
\end{definition}
Notice that the definition implies that the inverse of $A^{-1}$ is just $A$, so $(A^{-1})^{-1} = A$.

\begin{theorem}
	The inverse of a matrix, if it exists, is unique.
\end{theorem}
\begin{proof}
	Let $A$ be a matrix with $B$ and $C$ as inverses.
	Then
	\begin{align*}
		I &= AC \\
		BI &= B(AC) \\
		&= (BA)C \\
		&= IC \\
		B &= C.
	\end{align*}
	Thus, the two inverses are the same, as desired.
\end{proof}
This uniqueness result is what allows the notation $A^{-1}$ to make sense.

We can compute the inverse of a matrix $A$ by creating a matrix of the form $[A \mid I]$ and then performing operations like adding linear combinations of rows together to put this matrix in the form $[I \mid A^{-1}]$.
Each one of these operations (scalar multiplying at row, adding linear combinations of rows, swapping two rows) is a linear transformation.
So, when simplifying to the form where we have an identity matrix on the left, we've found a composition of operations that ``undoes'' the linear transformation represented by $A$.

\begin{example}
	Find the inverse of the following matrix:
	\begin{equation*}
		A = \begin{bmatrix}
			1 & 1 & -2 \\
			-1 & 2 & 0 \\
			0 & -1 & 1
		\end{bmatrix}.
	\end{equation*}
\end{example}
\begin{answer}
	Setting up $[A \mid I_3]$ and simplifying,
	\begin{align*}
		&\left[\begin{array}{ccc | ccc}
			1 & 1 & -2 & 1 & 0 & 0 \\
			-1 & 2 & 0 & 0 & 1 & 0 \\
			0 & -1 & 1 & 0 & 0 & 1
		\end{array}\right]
		\stackrel{R_2 = R_2 + R_1}{\to}
		\left[\begin{array}{ccc | ccc}
			1 & 1 & -2 & 1 & 0 & 0 \\
			0 & 3 & -2 & 1 & 1 & 0 \\
			0 & -1 & 1 & 0 & 0 & 1
		\end{array}\right]
		\stackrel{R_3 = R_2 + 3R_3}{\to}
		\left[\begin{array}{ccc | ccc}
			1 & 1 & -2 & 1 & 0 & 0 \\
			0 & 3 & -2 & 1 & 1 & 0 \\
			0 & 0 & 1  & 1 & 1 & 3
		\end{array}\right] \\
		&\stackrel{R_2 = R_2 + 2R_3}{\to}
		\left[\begin{array}{ccc | ccc}
			1 & 1 & -2 & 1 & 0 & 0 \\
			0 & 3 & 0  & 3 & 3 & 6 \\
			0 & 0 & 1  & 1 & 1 & 3
		\end{array}\right]
		\stackrel{R_2 = R_2 / 3}{\to}
		\left[\begin{array}{ccc | ccc}
			1 & 1 & -2 & 1 & 0 & 0 \\
			0 & 1 & 0  & 1 & 1 & 2 \\
			0 & 0 & 1  & 1 & 1 & 3
		\end{array}\right] 
		\stackrel{R_1 = R_1 + 2R_3}{\to}
		\left[\begin{array}{ccc | ccc}
			1 & 1 & 0 & 3 & 2 & 6 \\
			0 & 1 & 0 & 1 & 1 & 2 \\
			0 & 0 & 1 & 1 & 1 & 3
		\end{array}\right] \\
		&\stackrel{R_1 = R_1 - R_2}{\to}
		\left[\begin{array}{ccc | ccc}
			1 & 0 & 0 & 2 & 1 & 4 \\
			0 & 1 & 0 & 1 & 1 & 2 \\
			0 & 0 & 1 & 1 & 1 & 3
		\end{array}\right].
	\end{align*}
	So, we see that
	\begin{equation*}
		A^{-1} = \begin{bmatrix}
			2 & 1 & 4 \\
			1 & 1 & 2 \\
			1 & 1 & 3
		\end{bmatrix}.
	\end{equation*}
\end{answer}

\subsubsection{Properties}
Now that we know how to invert matrices, we might want to know whether we can invert sums and products of invertible matrices.
Let $A$ and $B$ be invertible matrices of the same size, and let $k$ be a scalar.
\begin{itemize}
	\item
	$A + B$ may not be invertible.
	For example, consider $A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ and $B = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$, which are both invertible.
	We see that	$A+B = \begin{bmatrix} 1 & 1 \\ 1 & 1\end{bmatrix}$, which is not invertible.
	\item
	$kA$ is invertible, and $(kA)^{-1} = \frac{1}{k}A^{-1}$.
	\item
	$AB$ is invertible, and $(AB)^{-1} = B^{-1}A^{-1}$.
\end{itemize}

Invertible matrices are those such that there exists and inverse matrix.
Since matrices represent linear transformations, it should come as no surprise that a linear transformation is invertible if and only if its matrix is invertible.
Since invertible linear transformations are bijections, we can determine when a system of equations has a unique solution.

\begin{theorem}
	If the $n \times n$ matrix $A$ is invertible, then the system $A\vec{x} = \vec{b}$ has a unique solution for any vector $\vec{b} \in \R^n$, namely $\vec{x} = A^{-1}\vec{b}$.
\end{theorem}